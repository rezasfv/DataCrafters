{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Populate our RDF database"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports and data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from pathlib import Path\n",
    "from IPython.display import clear_output\n",
    "\n",
    "#!pip install rdflib\n",
    "from rdflib import Graph, Literal, RDF, URIRef, Namespace, term\n",
    "from rdflib.namespace import FOAF, XSD\n",
    "\n",
    "#season parameters\n",
    "y = 2019\n",
    "shortSeas = \"1920\"\n",
    "fullSeas = str(y)+\"-\"+str(y+1)\n",
    "\n",
    "# parameters and URLs\n",
    "path = str(Path(os.path.abspath(os.getcwd())).parent.absolute())\n",
    "statsUrl = path + \"/inDepthSoccerStats/\"+fullSeas+\".csv\"\n",
    "statsFBrefUrl = path + \"/inDepthSoccerStats/transfermarkt_fbref_20\"+shortSeas+\".csv\"\n",
    "playersUrl = path + '/inDepthSoccerStats/players.csv'\n",
    "teamsUrl = path + '/inDepthSoccerStats/clubs.csv'\n",
    "appUrl = path + '/inDepthSoccerStats/appearances.csv'\n",
    "gamesUrl = path + '/inDepthSoccerStats/games.csv'\n",
    "\n",
    "# country codes\n",
    "countriesURL = path + '/inDepthSoccerStats/wikipedia-iso-country-codes.csv'\n",
    "\n",
    "# saving folder\n",
    "savePath =  path + '/rdf/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the CSV files in memory\n",
    "stats = pd.read_csv(statsUrl, sep=',', index_col='indCol')\n",
    "#these dataframes store data from Transfermarkt\n",
    "players = pd.read_csv(playersUrl, sep=',', index_col='player_id')\n",
    "teams = pd.read_csv(teamsUrl, sep=',', index_col='club_id')\n",
    "app = pd.read_csv(appUrl, sep=',', index_col='appearance_id')\n",
    "games = pd.read_csv(gamesUrl, sep=',', index_col='game_id')\n",
    "#FBref file used for completing some missing data. Available only for 3 seasons.\n",
    "if(y >= 2017):\n",
    "    statsFBref = pd.read_csv(statsFBrefUrl, sep=';', index_col='Column1', dtype={\"Attendance\": \"string\"})\n",
    "\n",
    "#load the country codes\n",
    "# we need to convert NaN values to something else otherwise NA strings are converted to NaN -> problem with Namibia\n",
    "countries = pd.read_csv(countriesURL, sep=',', index_col='English short name lower case', keep_default_na=False, na_values=['_'])\n",
    "\n",
    "#players.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parsing and matching utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "#!pip install unidecode\n",
    "#!pip install googlesearch-python\n",
    "from unidecode import unidecode\n",
    "from itertools import permutations\n",
    "from difflib import SequenceMatcher\n",
    "from googlesearch import search\n",
    "\n",
    "#string parsing\n",
    "\n",
    "def hyphenize(s):\n",
    "    return unidecode(s.lower().replace(\" \",\"-\").replace(\"&#039;\",\"'\"))\n",
    "\n",
    "def cleanChars(item):\n",
    "    return item.str.replace(\"ć\", \"c\").str.replace(\"ğ\",\"g\").str.replace(\"İ\",\"i\").str.replace(\"-scaron-\",\"s\")\n",
    "\n",
    "def cleanString(s):\n",
    "    return unidecode(s.replace(\"ć\", \"c\").replace(\"ğ\",\"g\").replace(\"İ\",\"i\"))\n",
    "    \n",
    "\n",
    "#permutations strategy\n",
    "\n",
    "def genSeqByLength(perm, length):\n",
    "    newName = \"\"\n",
    "    for j in range(0, length - 1):\n",
    "        newName = newName + perm[j] + \"-\"\n",
    "    newName = newName + perm[length - 1]\n",
    "    return newName\n",
    "\n",
    "def match_seq(splitS1, lis):\n",
    "    resultList = []\n",
    "    #iterate on all possible permutations\n",
    "    length = -1\n",
    "    if(len(lis) == 1):\n",
    "        length = len(lis[0].split(\"-\"))\n",
    "        uniqueItem = lis[0]\n",
    "    for perm in permutations(splitS1):\n",
    "        if(length != -1):\n",
    "            newName = genSeqByLength(perm, length)\n",
    "            if(uniqueItem == newName):\n",
    "                return [uniqueItem]\n",
    "        #consider all lengths from 2 to n\n",
    "        else:\n",
    "            for i in range(2, len(splitS1) + 1):\n",
    "                newName = genSeqByLength(perm, i)\n",
    "                for item in lis:\n",
    "                    if(item == newName):\n",
    "                        resultList.append(newName)\n",
    "    return resultList\n",
    "\n",
    "#multiple matches resolution\n",
    "\n",
    "def getAppsByID(ID, apps):\n",
    "    return np.size(apps[apps['player_id'] == ID], 0);\n",
    "\n",
    "#I: number of games in the season, candidate players, list describing single appearances\n",
    "#O: player from players file, or empty Series\n",
    "def solve_with_apps_approx(statsGames, somePlayers, appsCol, tol=5):\n",
    "    minDiff = 50\n",
    "    minInd = 0\n",
    "    for ind in somePlayers.index:\n",
    "        if(abs(statsGames - getAppsByID(ind, appsCol)) < minDiff):\n",
    "            minDiff = abs(statsGames - getAppsByID(ind, appsCol))\n",
    "            minInd = ind\n",
    "    if(minDiff <= tol):\n",
    "        player = somePlayers[somePlayers.index == minInd].iloc[0] \n",
    "        return player, minInd\n",
    "    else:\n",
    "        return pd.Series([]), -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Matching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matching teams from different sources\n",
    "This section is intended to create a dictionary to match team names in stats files to their Transfermarkt ID. This operation can be performed just once, as the dictionary can be dumped to a file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "statsTeamsSet = set()\n",
    "for n in range(2014, 2020):\n",
    "    tmpstatsUrl = path + \"/inDepthSoccerStats/\"+str(n)+\"-\"+str(n+1)+\".csv\"\n",
    "    tmpstats = pd.read_csv(tmpstatsUrl, sep=',', index_col='indCol')\n",
    "    for ind, row in tmpstats.iterrows():\n",
    "        currTeams = row['teams_played_for'].split(\",\")\n",
    "        statsTeamsSet.update(currTeams)\n",
    "\n",
    "statsTeams = list(statsTeamsSet)\n",
    "teamIDDict = dict()\n",
    "i = 0\n",
    "for statsTeam in statsTeams:\n",
    "    i += 1\n",
    "    maxS = 0\n",
    "    maxId = 0\n",
    "    for tind, trow in teams.iterrows():\n",
    "        sm = SequenceMatcher(None, statsTeam, trow['name'])\n",
    "        sim = sm.ratio()\n",
    "        if(sim > maxS):\n",
    "            maxS = sim\n",
    "            maxId = tind\n",
    "\n",
    "    if(maxS < 0.95):\n",
    "        splitURL = next(search(statsTeam+\" transfermarkt startseite verein\", num_results=1)).split(\"/\")\n",
    "        #some teams contain numbers in their name, so we need to take only the suffix of the URL\n",
    "        trID = splitURL[len(splitURL) - 1]\n",
    "        if(trID != \"\"):\n",
    "            if(len(teams[teams.index == int(trID)]) == 1):\n",
    "                maxId = int(trID)\n",
    "                print(\"{:3d}\".format(i)+\" out of \"+\"{:3d}\".format(len(statsTeams))+\" GOOGLE: \"+statsTeam+\" --> \"+teams.at[maxId, 'name'])\n",
    "            else:\n",
    "                print(\"Invalid ID extracted from \"+URL)\n",
    "        else:\n",
    "            print(\"No ID in URL \"+URL)\n",
    "\n",
    "    teamIDDict[statsTeam] = maxId"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "teamsSet = set()\n",
    "for n in range(14, 20):\n",
    "    tmpUrl = path + \"/inDepthSoccerStats/ladders\"+str(n)+str(n+1)+\".csv\"\n",
    "    tmp = pd.read_csv(tmpUrl, sep=';')\n",
    "    for ind, row in tmp.iterrows():\n",
    "        teamsSet.add(row['Team'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We dump the dictionary to a file in the working directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('teamdict.pkl', 'wb') as f:\n",
    "    pickle.dump(teamIDDict, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matching players from different sources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load the dictionary with team IDs which is saved in the working directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('teamdict.pkl', 'rb') as f:\n",
    "    teamIDDict = pickle.load(f)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We add 3 new columns to the dataframe to insert also Transfermarkt IDs of the player and the teams he has played for."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats['trID'] = [0] * len(stats)\n",
    "stats['team0ID'] = [0] * len(stats)\n",
    "stats['team1ID'] = [0] * len(stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "#select only months and years from appearance dates\n",
    "appMonth = app['date'].str.split(\"/\").str[1]\n",
    "appYear = app['date'].str.split(\"/\").str[2]\n",
    "#select appearances from this season\n",
    "isThisSeas = ((appYear == str(y)) & (appMonth >= \"08\")) | ((appYear == str(y+1)) & (appMonth <= \"06\"))\n",
    "appThisSeas = app[isThisSeas]\n",
    "playerCodes = cleanChars(players['player_code'])\n",
    "\n",
    "#iterate on stats file\n",
    "statsRows = np.size(stats, 0);\n",
    "toComplRows = len(stats[stats['trID'] == 0])\n",
    "exact_matches = no_matches = resolved_google = resolved_permS = resolved_permP = resolved_max_sim = resolved_pres = i = 0\n",
    "for index, row in stats.iterrows():\n",
    "    if(row['trID'] == 0):\n",
    "        i += 1\n",
    "        mode = \"NONE\"\n",
    "        player = pd.Series([])\n",
    "        statsName = hyphenize(row['player_name']).replace(\"'\",\"\")\n",
    "        \n",
    "        matchedPlayers = players[playerCodes == statsName]\n",
    "    \n",
    "        #multiple rows with same name in stats mapped to a single player are ok (he has changed team during the season)\n",
    "        if(np.size(matchedPlayers, 0) > 0):\n",
    "            mode = \"MATCH\"\n",
    "           \n",
    "        if(mode == \"NONE\"):\n",
    "            #split name in stats and use permutations strategy\n",
    "            splitStatsName = statsName.split(\"-\")\n",
    "            if(len(splitStatsName) >= 2):\n",
    "                matchedCodes = match_seq(splitStatsName, playerCodes)\n",
    "                if(len(matchedCodes) > 0):\n",
    "                    matchedPlayers = players[playerCodes.isin(matchedCodes)]\n",
    "                    mode = \"PERM1\"\n",
    "    \n",
    "        if(mode == \"NONE\"):\n",
    "            maxSim = 0\n",
    "            maxC = \"\"\n",
    "            for c in playerCodes:\n",
    "                sm = SequenceMatcher(None, statsName, c)\n",
    "                #do not proceed if the upper bound is too small\n",
    "                if(sm.real_quick_ratio() >= 0.5):\n",
    "                    #remember: similarity is not commutative\n",
    "                    simm = sm.ratio()                  \n",
    "                    #if sim is big enough, try permutation strategy with name from players file\n",
    "                    if(simm >= 0.6):\n",
    "                        splitC = c.split(\"-\")\n",
    "                        if(len(splitC) >= len(splitStatsName) and len(splitStatsName) >= 2):\n",
    "                            matchedCodes = match_seq(splitC, [statsName])\n",
    "                            if(len(matchedCodes) > 0):\n",
    "                                newMatchedPlayers = players[players['player_code'] == c]\n",
    "                                matchedPlayers = pd.concat([matchedPlayers, newMatchedPlayers])\n",
    "                    if(simm > maxSim):\n",
    "                        maxSim = simm\n",
    "                        maxC = c\n",
    "    \n",
    "            if(maxSim >= 0.95):\n",
    "                matchedPlayers = players[playerCodes == maxC]\n",
    "                mode = \"MAXSIM\"\n",
    "            elif(np.size(matchedPlayers, 0) > 0):\n",
    "                mode = \"PERM2\"\n",
    "    \n",
    "        #managing results of any method\n",
    "        matches = np.size(matchedPlayers, 0)\n",
    "        if(matches == 1):\n",
    "            player = matchedPlayers.iloc[0]\n",
    "            trID = matchedPlayers.index[0]\n",
    "        if(matches > 1):\n",
    "            player, trID = solve_with_apps_approx(row['games'], matchedPlayers, appThisSeas, 3)\n",
    "            if(trID == -1):\n",
    "                mode = \"NONE\"\n",
    "            else:\n",
    "                mode = \"PRES\"\n",
    "        if(mode == \"NONE\"):\n",
    "            splitURL = next(search(row['player_name']+\" \"+row['teams_played_for']+\" transfermarkt profil spieler\", num_results=1)).split(\"/\")\n",
    "            trID = splitURL[len(splitURL) - 1]\n",
    "            if(trID != \"\"):\n",
    "                urlPlayers = players[players.index == int(trID)]\n",
    "                if(len(urlPlayers) != 0):\n",
    "                    mode = \"GOOGLE\"\n",
    "                    player = urlPlayers.iloc[0]\n",
    "                else:\n",
    "                    print(\"!!! Invalid ID \"+trID+\" extracted from \"+str(splitURL))\n",
    "                    mode = \"NONE\"\n",
    "            else:\n",
    "                print(\"!!! No ID in URL \"+str(splitURL))\n",
    "                mode = \"NONE\"\n",
    "        \n",
    "        \n",
    "        if(mode == \"NONE\"):\n",
    "            no_matches += 1\n",
    "        elif(mode == \"PERM1\"):\n",
    "            resolved_permS += 1\n",
    "        elif(mode == \"PERM2\"):\n",
    "            resolved_permP += 1\n",
    "        elif(mode == \"GOOGLE\"):\n",
    "            resolved_google += 1\n",
    "        elif(mode == \"MATCH\"):\n",
    "            exact_matches += 1\n",
    "        elif(mode == \"MAXSIM\"):\n",
    "            resolved_max_sim += 1\n",
    "        elif(mode == \"PRES\"):\n",
    "            resolved_pres += 1\n",
    "    \n",
    "        if(mode == \"NONE\"):\n",
    "            print(\"{:4d}\".format(i)+\" out of \"+str(toComplRows)+\" NONE  : \"+statsName+\", \"+row['teams_played_for']+\", matches: \"+str(matches))\n",
    "        elif(mode != \"MATCH\"):\n",
    "            print(\"{:4d}\".format(i)+\" out of \"+str(toComplRows)+\" \"+mode.ljust(6)+\": \"+statsName+\" --> \"+player['player_code'])\n",
    "    \n",
    "        if(mode != \"NONE\"):\n",
    "            stats.at[index, 'trID'] = int(trID)\n",
    "            teamList = row['teams_played_for'].split(\",\")\n",
    "            stats.at[index, 'team0ID'] = int(teamIDDict[teamList[0]])\n",
    "            if(len(teamList) == 2):\n",
    "                stats.at[index, 'team1ID'] = int(teamIDDict[teamList[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print statistics\n",
    "print(\"   --- STATISTICS ---\")\n",
    "tot_matches = exact_matches + + resolved_google + resolved_permS + resolved_permP + resolved_max_sim + resolved_pres\n",
    "print(\"Total matches:                            \"+\"{:5d}\".format(tot_matches)+\" -- percentage: \" + \"{:.2f}%\".format(tot_matches*100/statsRows))\n",
    "print(\"  ---> exact matches:                     \"+\"{:5d}\".format(exact_matches)+\" -- percentage: \" + \"{:.2f}%\".format(exact_matches*100/statsRows)) \n",
    "print(\"  ---> resolved permutating statsName:    \"+\"{:5d}\".format(resolved_permS)+\" -- percentage: \" + \"{:.2f}%\".format(resolved_permS*100/statsRows))\n",
    "print(\"  ---> resolved permutating player code:  \"+\"{:5d}\".format(resolved_permP)+\" -- percentage: \" + \"{:.2f}%\".format(resolved_permP*100/statsRows))\n",
    "print(\"  ---> resolved with max sim.:            \"+\"{:5d}\".format(resolved_max_sim)+\" -- percentage: \" + \"{:.2f}%\".format(resolved_max_sim*100/statsRows))\n",
    "print(\"  ---> resolved with apps:                \"+\"{:5d}\".format(resolved_pres)+\" -- percentage: \" + \"{:.2f}%\".format(resolved_pres*100/statsRows))\n",
    "print(\"  ---> resolved with google:              \"+\"{:5d}\".format(resolved_google)+\" -- percentage: \" + \"{:.2f}%\".format(resolved_google*100/statsRows))\n",
    "print(\"No matches:                               \"+\"{:5d}\".format(no_matches)+\" -- percentage: \" + \"{:.2f}%\".format(no_matches*100/statsRows))\n",
    "print(\"  ---> zero matches found:                \"+\"{:5d}\".format(no_matches)+\" -- percentage: \" + \"{:.2f}%\".format((no_matches)*100/statsRows))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the dataframe contains TM IDs, which will be used like foreign keys for matching purposes. Therefore, we save the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the dataframe\n",
    "stats.to_csv(\"stats\"+shortSeas+\"_IDs.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Completing and correcting statistics in corner cases\n",
    "We need to manage the fact some rows contain total information about a player switching team in the same league during the season. <br>\n",
    "We can use information in the FBref files to complete our data; we have observed that, in this situation, it contains correct information only for the row of the two which has lower index: some statistics in the second row can be therefore corrected by subtracting the ones in the first row from the total ones. <br>\n",
    "We also delete some inconsistencies arising because of different calculations or errors in the second dataset, by setting to 0 negative values.\n",
    "Major statistics (e.g. goals) show very few such visible errors, and are corrected manually.\n",
    "\n",
    "Observe that we load stats to make this cell idempotent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we will use copies of row of the df, so we silence the warnings that would arise\n",
    "pd.options.mode.chained_assignment = None\n",
    "\n",
    "statsFBref = pd.read_csv(statsFBrefUrl, sep=';', index_col='Column1', dtype={\"Attendance\": \"string\"})\n",
    "statsFBref['player'] = [unidecode(name) for name in statsFBref['player']]\n",
    "\n",
    "stats = pd.read_csv('stats'+shortSeas+'_IDs.csv', sep=',', index_col='indCol')\n",
    "\n",
    "m = nm = 0\n",
    "newRows = []\n",
    "for ind, row in stats.iterrows():\n",
    "    if(row['team1ID'] != 0):\n",
    "        player = players.loc[row['trID']]\n",
    "        fbmatch = statsFBref[statsFBref['player'] == unidecode(player['name'])]\n",
    "        if(len(fbmatch) == 2):\n",
    "            m += 1\n",
    "            team0ID, team1ID = row['team0ID'], row['team1ID']\n",
    "            team0 = teams.loc[team0ID]['name']\n",
    "\n",
    "            #select the index pointing to wrong row\n",
    "            if(fbmatch.index[0] < fbmatch.index[1]):\n",
    "                corrInd, wrongInd = fbmatch.index\n",
    "            else:\n",
    "                wrongInd, corrInd = fbmatch.index\n",
    "\n",
    "            sm0 = SequenceMatcher(None, team0, fbmatch.loc[corrInd]['squad'])\n",
    "            sm1 = SequenceMatcher(None, team0, fbmatch.loc[wrongInd]['squad'])\n",
    "\n",
    "            #if the team corresponding to first ID matches with fbref row with index wrongInd, swap team IDs\n",
    "            if(sm0.ratio() < sm1.ratio()):\n",
    "                team0ID, team1ID = row['team1ID'], row['team0ID']\n",
    "\n",
    "            corrRow = stats.loc[ind]\n",
    "            wrongRow = stats.loc[ind]\n",
    "            \n",
    "            corrRow.at['teams_played_for'] = teams.loc[team0ID]['name']\n",
    "            corrRow.at['team0ID'] = team0ID\n",
    "            corrRow.at['team1ID'] = 0\n",
    "            corrRow.at['games'] = statsFBref.at[corrInd, 'games']\n",
    "            corrRow.at['goals'] = statsFBref.at[corrInd, 'goals']\n",
    "            corrRow.at['minutes_played'] = statsFBref.at[corrInd, 'minutes']\n",
    "            corrRow.at['npg'] = statsFBref.at[corrInd, 'goals'] - statsFBref.at[corrInd, 'pens_made']\n",
    "            corrRow.at['assists'] = statsFBref.at[corrInd, 'assists']\n",
    "            corrRow.at['xG'] = statsFBref.at[corrInd, 'xg']\n",
    "            corrRow.at['xG90'] = corrRow['xG'] / corrRow['minutes_played'] * 90\n",
    "            corrRow.at['npxG'] = statsFBref.at[corrInd, 'npxg']\n",
    "            corrRow.at['npxG90'] = corrRow['npxG'] / corrRow['minutes_played'] * 90\n",
    "            corrRow.at['xA'] = statsFBref.at[corrInd, 'xa']\n",
    "            corrRow.at['xA90'] = corrRow['xA'] / corrRow['minutes_played'] * 90\n",
    "            corrRow.at['yellow_cards'] = statsFBref.at[corrInd, 'cards_yellow']\n",
    "            corrRow.at['red_cards'] = statsFBref.at[corrInd, 'cards_red']\n",
    "            corrRow.at['shots'] = statsFBref.at[corrInd, 'shots_total']\n",
    "\n",
    "            wrongRow.at['teams_played_for'] = teams.loc[team1ID]['name']\n",
    "            wrongRow.at['team0ID'] = team1ID\n",
    "            wrongRow.at['team1ID'] = 0\n",
    "            wrongRow.at['games'] = row['games'] - corrRow['games']\n",
    "            if(wrongRow['games'] < 0):\n",
    "                print(wrongRow['player_name']+\" games\")\n",
    "            wrongRow.at['goals'] = row['goals'] - corrRow['goals']\n",
    "            if(wrongRow['goals'] < 0):\n",
    "                print(wrongRow['player_name']+\" goals\")\n",
    "            wrongRow.at['minutes_played'] = max(row['minutes_played'] - corrRow['minutes_played'], 1)\n",
    "            wrongRow.at['npg'] = row['npg'] - corrRow['npg']\n",
    "            if(wrongRow['npg'] < 0):\n",
    "                print(wrongRow['player_name']+\" npg\")\n",
    "            wrongRow.at['assists'] = max(row['assists'] - corrRow['assists'], 0)\n",
    "            wrongRow.at['xG'] = max(row['xG'] - corrRow['xG'], 0)\n",
    "            wrongRow.at['xG90'] = wrongRow['xG'] / wrongRow['minutes_played'] * 90\n",
    "            wrongRow.at['npxG'] = max(row['npxG'] - corrRow['npxG'], 0)\n",
    "            wrongRow.at['npxG90'] = wrongRow['npxG'] / wrongRow['minutes_played'] * 90\n",
    "            wrongRow.at['xA'] = max(row['xA'] - corrRow['xA'], 0)\n",
    "            wrongRow.at['xA90'] = wrongRow['xA'] / wrongRow['minutes_played'] * 90\n",
    "            wrongRow.at['yellow_cards'] = max(row['yellow_cards'] - corrRow['yellow_cards'], 0)\n",
    "            wrongRow.at['red_cards'] = max(row['red_cards'] - corrRow['red_cards'], 0)\n",
    "            wrongRow.at['shots'] = max(row['shots'] - corrRow['shots'], 0)\n",
    "\n",
    "            #we lack of data for these columns, so we mark this lack for later\n",
    "            corrRow.at['xGBuildup'] = corrRow['xGChain'] = wrongRow.at['xGBuildup'] = wrongRow['xGChain'] = -1\n",
    "            corrRow.at['key_passes'] = wrongRow.at['key_passes'] = -1\n",
    "            \n",
    "            newRows.append(corrRow.values)\n",
    "            newRows.append(wrongRow.values)\n",
    "\n",
    "        else:\n",
    "            nm += 1\n",
    "            print(row['player_name'])\n",
    "            \n",
    "print(\"Could match: \"+str(m)+\" could not: \"+str(nm))\n",
    "#create the new dataframe to be appended to the old one\n",
    "startInd = stats.index[len(stats) - 1] + 1\n",
    "newStats = pd.DataFrame(newRows, columns=stats.columns, index=range(startInd,startInd+len(newRows)))\n",
    "stats = pd.concat([stats, newStats])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the dataframe\n",
    "stats.to_csv(\"stats\"+shortSeas+\"_IDs.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is also a different situation: the dataset contains two rows for each player who has switched league during a single season. <br>\n",
    "We add a row representing aggregated statistics in these cases: it can be useful for query purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "pd.options.mode.chained_assignment = None\n",
    "\n",
    "shortSeas = \"1920\"\n",
    "stats = pd.read_csv('stats'+shortSeas+'_IDs.csv', sep=',', index_col='indCol')\n",
    "\n",
    "#we create a set with all the tm IDs corresponding to exactly to rows, so to the situation described above\n",
    "toAggr = set()\n",
    "for ind, row in stats.iterrows():\n",
    "    if(len(stats[stats['trID'] == row['trID']]) == 2):\n",
    "        toAggr.add(row['trID'])\n",
    "\n",
    "newRows = []\n",
    "for id in toAggr:\n",
    "    toAggrRows = stats[stats['trID'] == id]\n",
    "    newRow = toAggrRows.iloc[0]\n",
    "    row1 = toAggrRows.iloc[1]\n",
    "    newRow.at['teams_played_for'] += (\",\"+row1['teams_played_for'])\n",
    "    newRow.at['games'] += row1['games']\n",
    "    newRow.at['minutes_played'] += row1['minutes_played']\n",
    "    newRow.at['goals'] += row1['goals']\n",
    "    newRow.at['npg'] += row1['npg']\n",
    "    newRow.at['assists'] += row1['assists']\n",
    "    newRow.at['xG'] += row1['xG']\n",
    "    newRow.at['xA'] += row1['xA']\n",
    "    newRow.at['npxG'] += row1['npxG']\n",
    "    newRow.at['shots'] += row1['shots']\n",
    "    newRow.at['key_passes'] += row1['key_passes']\n",
    "    newRow.at['yellow_cards'] += row1['yellow_cards']\n",
    "    newRow.at['red_cards'] += row1['red_cards']\n",
    "    newRow.at['xGBuildup'] += row1['xGBuildup']\n",
    "    newRow.at['xGChain'] += row1['xGChain']\n",
    "    newRow.at['team1ID'] = row1['team0ID']    \n",
    "    newRow.at['xG90'] *= (90/newRow.at['minutes_played'])\n",
    "    newRow.at['xA90'] *= (90/newRow.at['minutes_played'])\n",
    "    newRow.at['npxG90'] *= (90/newRow.at['minutes_played'])\n",
    "    \n",
    "    newRows.append(newRow.values)\n",
    "\n",
    "startInd = stats.index[len(stats) - 1] + 1\n",
    "newStats = pd.DataFrame(newRows, columns=stats.columns, index=range(startInd,startInd+len(newRows)))\n",
    "stats = pd.concat([stats, newStats])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save the dataframe\n",
    "stats.to_csv(\"stats\"+shortSeas+\"_IDs.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Namespaces and prefixes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This snippet is repeated below to save serializations in different files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct the soccer stats ontology namespace\n",
    "#CNS = Namespace(\"http://eulersharp.sourceforge.net/2003/03swap/countries#\")\n",
    "DCSSO = Namespace(\"http://www.dei.unipd.it/db2/dcsso#\")\n",
    "\n",
    "#create the graph\n",
    "g = Graph()\n",
    "\n",
    "# Bind the namespaces to a prefix for more readable output\n",
    "g.bind(\"foaf\", FOAF)\n",
    "g.bind(\"xsd\", XSD)\n",
    "#g.bind(\"countries\", CNS)\n",
    "g.bind(\"dcsso\", DCSSO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Graph population"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Countries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "for ind, row in countries.iterrows():\n",
    "    country = URIRef(DCSSO[row['Alpha-2 code']])\n",
    "    g.add((country, RDF.type, DCSSO.Country))\n",
    "    g.add((country, FOAF.name, Literal(cleanString(ind), datatype=XSD.string)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Leagues\n",
    "Leagues are added manually because we need to store only five using very limited information from the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "SerieA = URIRef(DCSSO[\"IT1\"])\n",
    "g.add((SerieA, RDF.type, DCSSO.League))\n",
    "g.add((SerieA, FOAF['name'], Literal(\"Serie A\", datatype=XSD.string)))\n",
    "g.add((SerieA, DCSSO['hasCountry'], URIRef(DCSSO[\"IT\"])))\n",
    "\n",
    "Ligue1 = URIRef(DCSSO[\"FR1\"])\n",
    "g.add((Ligue1, RDF.type, DCSSO.League))\n",
    "g.add((Ligue1, FOAF['name'], Literal(\"Ligue 1\", datatype=XSD.string)))\n",
    "g.add((Ligue1, DCSSO['hasCountry'], URIRef(DCSSO[\"FR\"])))\n",
    "\n",
    "LaLiga = URIRef(DCSSO[\"ES1\"])\n",
    "g.add((LaLiga, RDF.type, DCSSO.League))\n",
    "g.add((LaLiga, FOAF['name'], Literal(\"LaLiga\", datatype=XSD.string)))\n",
    "g.add((LaLiga, DCSSO['hasCountry'], URIRef(DCSSO[\"ES\"])))\n",
    "\n",
    "Premier = URIRef(DCSSO[\"GB1\"])\n",
    "g.add((Premier, RDF.type, DCSSO.League))\n",
    "g.add((Premier, FOAF['name'], Literal(\"Premier League\", datatype=XSD.string)))\n",
    "g.add((Premier, DCSSO['hasCountry'], URIRef(DCSSO[\"GB\"])))\n",
    "\n",
    "Bundesliga = URIRef(DCSSO[\"L1\"])\n",
    "g.add((Bundesliga, RDF.type, DCSSO.League))\n",
    "g.add((Bundesliga, FOAF['name'], Literal(\"Bundesliga\", datatype=XSD.string)))\n",
    "g.add((Bundesliga, DCSSO['hasCountry'], URIRef(DCSSO[\"DE\"])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We serialize the graph and save the output with turtle syntax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# print all the data in the Turtle format\n",
    "print(\"--- saving serialization ---\")\n",
    "with open(savePath + 'countries_leagues.ttl', 'w') as file:\n",
    "    file.write(g.serialize(format='turtle'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now clean the graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct the soccer stats ontology namespace\n",
    "DCSSO = Namespace(\"http://www.dei.unipd.it/db2/dcsso#\")\n",
    "\n",
    "#create the graph\n",
    "g = Graph()\n",
    "\n",
    "# Bind the namespaces to a prefix for more readable output\n",
    "g.bind(\"foaf\", FOAF)\n",
    "g.bind(\"xsd\", XSD)\n",
    "g.bind(\"dcsso\", DCSSO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Teams"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load the dictionary with team IDs, to understand which teams need to be stored in the DB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "with open('teamdict.pkl', 'rb') as f:\n",
    "    teamIDDict = pickle.load(f)\n",
    "\n",
    "for tID in set(teamIDDict.values()):\n",
    "    team = teams.loc[tID]\n",
    "    Team = URIRef(DCSSO[\"team\"+str(tID)])\n",
    "    g.add((Team, RDF.type, DCSSO.Team))\n",
    "    g.add((Team, FOAF.name, Literal(cleanString(team['name']), datatype=XSD.string)))\n",
    "    g.add((Team, DCSSO['participatesIn'], URIRef(DCSSO[team['domestic_competition_id']])))\n",
    "    g.add((Team, DCSSO['stadium'], Literal(cleanString(team['stadium_name']), datatype=XSD.string)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "# print all the data in the Turtle format\n",
    "print(\"--- saving serialization ---\")\n",
    "with open(savePath + 'teams.ttl', 'w') as file:\n",
    "    file.write(g.serialize(format='turtle'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct the soccer stats ontology namespace\n",
    "DCSSO = Namespace(\"http://www.dei.unipd.it/db2/dcsso#\")\n",
    "\n",
    "#create the graph\n",
    "g = Graph()\n",
    "\n",
    "# Bind the namespaces to a prefix for more readable output\n",
    "g.bind(\"foaf\", FOAF)\n",
    "g.bind(\"xsd\", XSD)\n",
    "g.bind(\"dcsso\", DCSSO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "#### Team participations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "for year in range(2014, 2020):\n",
    "    ss = str(year - 2000)+str(year - 1999)\n",
    "    tmpUrl = path + \"/inDepthSoccerStats/ladders\"+ss+\".csv\"\n",
    "    standings = pd.read_csv(tmpUrl, sep=';')\n",
    "    for ind,row in standings.iterrows():\n",
    "        tID = teamIDDict[row['Team']]\n",
    "        if(ind == 0 or teams.loc[oldtID]['domestic_competition_id'] != teams.loc[tID]['domestic_competition_id']):\n",
    "            pos = 1\n",
    "        else:\n",
    "            pos += 1\n",
    "        Team = URIRef(DCSSO[\"team\"+str(tID)])\n",
    "        Participation = URIRef(DCSSO[\"part\"+str(tID)+\"s\"+str(year)])\n",
    "        g.add((Participation, RDF.type, DCSSO.SeasonalParticipation))\n",
    "        g.add((Team, DCSSO['hasParticipation'], Participation))\n",
    "        g.add((Participation, DCSSO['season'], Literal(year, datatype=XSD.int)))\n",
    "        g.add((Participation, DCSSO['finalPosition'], Literal(pos, datatype=XSD.int)))\n",
    "        g.add((Participation, DCSSO['games'], Literal(row['M'], datatype=XSD.int)))\n",
    "        g.add((Participation, DCSSO['wins'], Literal(row['W'], datatype=XSD.int)))\n",
    "        g.add((Participation, DCSSO['draws'], Literal(row['D'], datatype=XSD.int)))\n",
    "        g.add((Participation, DCSSO['losses'], Literal(row['L'], datatype=XSD.int)))\n",
    "        g.add((Participation, DCSSO['goals'], Literal(row['G'], datatype=XSD.int)))\n",
    "        g.add((Participation, DCSSO['goalsAgainst'], Literal(row['GA'], datatype=XSD.int)))\n",
    "        g.add((Participation, DCSSO['points'], Literal(row['PTS'], datatype=XSD.int)))\n",
    "        g.add((Participation, DCSSO['xG'], Literal(row['xG'], datatype=XSD.double)))\n",
    "        g.add((Participation, DCSSO['xGA'], Literal(row['xGA'], datatype=XSD.double)))\n",
    "        g.add((Participation, DCSSO['xPoints'], Literal(row['xPTS'], datatype=XSD.double)))\n",
    "        \n",
    "\n",
    "        oldtID = tID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "# print all the data in the Turtle format\n",
    "print(\"--- saving serialization ---\")\n",
    "with open(savePath + 'team_part.ttl', 'w') as file:\n",
    "    file.write(g.serialize(format='turtle'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct the soccer stats ontology namespace\n",
    "DCSSO = Namespace(\"http://www.dei.unipd.it/db2/dcsso#\")\n",
    "\n",
    "#create the graph\n",
    "g = Graph()\n",
    "\n",
    "# Bind the namespaces to a prefix for more readable output\n",
    "g.bind(\"foaf\", FOAF)\n",
    "g.bind(\"xsd\", XSD)\n",
    "g.bind(\"dcsso\", DCSSO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Players"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100% completed...\n",
      "CPU times: total: 2.94 s\n",
      "Wall time: 6.73 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "players['first_name'] = players['first_name'].fillna(\"\")\n",
    "players['last_name'] = players['last_name'].fillna(\"\")\n",
    "players['country_of_citizenship'] = players['country_of_citizenship'].fillna(\"\")\n",
    "players['height_in_cm'] = players['height_in_cm'].fillna(0)\n",
    "players['date_of_birth'] = players['date_of_birth'].fillna(\"\")\n",
    "\n",
    "#collect all player IDs in the files produced above\n",
    "plIDSet = set()\n",
    "for n in range(14, 20):\n",
    "    tmpstats = pd.read_csv('stats'+str(n)+str(n+1)+'_IDs.csv', sep=',')\n",
    "    for ind, row in tmpstats.iterrows():\n",
    "        if(row['trID'] != 0):\n",
    "            plIDSet.add(row['trID'])\n",
    "\n",
    "c = 0\n",
    "for id in plIDSet:\n",
    "    c+=1\n",
    "    if(int(100*(c-1)/len(plIDSet)) < int(100*c/len(plIDSet))):\n",
    "        clear_output(wait=True)\n",
    "        print(str(int(100*c/len(plIDSet)))+\"% completed.\"+\".\"*(c%3))\n",
    "    player = players.loc[id]\n",
    "    # the node has the namespace + the transfermarkt ID as URI\n",
    "    ref = \"player\"+str(id)\n",
    "    Footballer = URIRef(DCSSO[ref])\n",
    "    g.add((Footballer, RDF.type, DCSSO.Footballer))\n",
    "    if(player['first_name'] != \"\"):\n",
    "        g.add((Footballer, FOAF['firstName'], Literal(cleanString(player['first_name']), datatype=XSD.string)))\n",
    "    if(player['last_name'] != \"\"):\n",
    "        g.add((Footballer, FOAF['familyName'], Literal(cleanString(player['last_name']), datatype=XSD.string)))\n",
    "    if(player['date_of_birth'] != \"\"):\n",
    "        g.add((Footballer, DCSSO['birthdate'], Literal(player['date_of_birth'][:4], datatype=XSD.int)))\n",
    "    g.add((Footballer, FOAF['name'], Literal(cleanString(player['name']), datatype=XSD.string)))\n",
    "    if(player['country_of_citizenship'] != \"\"):\n",
    "        g.add((Footballer, DCSSO['hasCitizenship'], DCSSO[countries.loc[player['country_of_citizenship']]['Alpha-2 code']]))\n",
    "    if(player['height_in_cm'] != 0 and np.isnan(player['height_in_cm']) == False):\n",
    "        g.add((Footballer, DCSSO['height'], Literal(player['height_in_cm'], datatype=XSD.double)))\n",
    "\n",
    "    if(player['position'] != \"Missing\"):\n",
    "        subPosition = player['sub_position'].replace(\" \", \"\").replace(\"-\", \"\")\n",
    "        if(player['position'] == \"Goalkeeper\" or player['position'] == \"Defender\"):\n",
    "            g.add((Footballer, DCSSO['position'], DCSSO[player['position']]))\n",
    "        elif(player['position'] == \"Midfield\"):\n",
    "            g.add((Footballer, DCSSO['position'], DCSSO[\"Midfielder\"]))\n",
    "            subPosition += \"er\"\n",
    "        else:\n",
    "            g.add((Footballer, DCSSO['position'], DCSSO[\"Forward\"]))\n",
    "    \n",
    "        g.add((Footballer, DCSSO['subPosition'], DCSSO[subPosition]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- saving serialization ---\n",
      "CPU times: total: 1.66 s\n",
      "Wall time: 3.13 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# print all the data in the Turtle format\n",
    "print(\"--- saving serialization ---\")\n",
    "with open(savePath + 'players.ttl', 'w') as file:\n",
    "    file.write(g.serialize(format='turtle'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Construct the soccer stats ontology namespace\n",
    "DCSSO = Namespace(\"http://www.dei.unipd.it/db2/dcsso#\")\n",
    "\n",
    "#create the graph\n",
    "g = Graph()\n",
    "\n",
    "# Bind the namespaces to a prefix for more readable output\n",
    "g.bind(\"foaf\", FOAF)\n",
    "g.bind(\"xsd\", XSD)\n",
    "g.bind(\"dcsso\", DCSSO)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we finally store the statistics available for each player in a season. <br>\n",
    "If there are more rows for the same player"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Players with value: 2796 -- no value: 29\n",
      "CPU times: total: 2.28 s\n",
      "Wall time: 6.38 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "y=2019\n",
    "shortSeas = str(y-2000)+str(y-1999)\n",
    "stats = pd.read_csv('stats'+str(shortSeas)+'_IDs.csv', sep=',')\n",
    "val = pd.read_csv(path+\"/inDepthSoccerStats/player_valuations\"+shortSeas+\".csv\", sep=',', index_col='player_id')\n",
    "\n",
    "yes = no = 0\n",
    "for index, row in stats.iterrows():\n",
    "    tmId = str(row['trID'])\n",
    "    if(tmId == \"0\"):\n",
    "        continue\n",
    "    # the node has the namespace + the transfermarkt ID as URI; we don't create the node for the footballer here\n",
    "    ref = \"player\"+tmId\n",
    "    Footballer = URIRef(DCSSO[ref])\n",
    "\n",
    "    teamIDs = str(row['team0ID'])\n",
    "    if(row['team1ID'] != 0):\n",
    "        teamIDs += \"_\"+str(row['team1ID'])\n",
    "        MembClassURI = DCSSO.SeasonalAggrMembership\n",
    "    else:\n",
    "        MembClassURI = DCSSO.SeasonalMembership\n",
    "    Memb = URIRef(DCSSO[\"memb\"+tmId+\"s\"+str(y)+\"t\"+teamIDs])\n",
    "    g.add((Memb, RDF.type, MembClassURI))\n",
    "    g.add((Footballer, DCSSO['hasMembership'], Memb))\n",
    "    g.add((Memb, DCSSO['season'], Literal(y, datatype=XSD.int)))\n",
    "    g.add((Memb, DCSSO['forTeam'], URIRef(DCSSO[\"team\"+str(row['team0ID'])])))\n",
    "    if(row['team1ID'] != 0):\n",
    "        g.add((Memb, DCSSO['forTeam'], URIRef(DCSSO[\"team\"+str(row['team1ID'])])))\n",
    "    if row['trID'] in val.index:\n",
    "        val_eur = val.loc[row['trID']]['market_value_in_eur']\n",
    "        g.add((Memb, DCSSO['tmValue'], Literal(val_eur, datatype=XSD.int)))\n",
    "        yes += 1\n",
    "    else:\n",
    "        no += 1\n",
    "        if(row['games'] >= 10):\n",
    "            print(row['player_name'])\n",
    "\n",
    "    #statistics\n",
    "    g.add((Memb, DCSSO['games'], Literal(row['games'], datatype=XSD.int)))\n",
    "    g.add((Memb, DCSSO['minutes'], Literal(row['minutes_played'], datatype=XSD.int)))\n",
    "    g.add((Memb, DCSSO['goals'], Literal(row['goals'], datatype=XSD.int)))\n",
    "    g.add((Memb, DCSSO['npg'], Literal(row['npg'], datatype=XSD.int)))\n",
    "    g.add((Memb, DCSSO['assists'], Literal(row['assists'], datatype=XSD.int)))\n",
    "    g.add((Memb, DCSSO['shots'], Literal(row['shots'], datatype=XSD.int)))\n",
    "    g.add((Memb, DCSSO['yellowCards'], Literal(row['yellow_cards'], datatype=XSD.int)))\n",
    "    g.add((Memb, DCSSO['redCards'], Literal(row['red_cards'], datatype=XSD.int)))\n",
    "    \n",
    "    g.add((Memb, DCSSO['xG'], Literal(row['xG'], datatype=XSD.double)))\n",
    "    g.add((Memb, DCSSO['xA'], Literal(row['xA'], datatype=XSD.double)))\n",
    "    g.add((Memb, DCSSO['npxG'], Literal(row['npxG'], datatype=XSD.double)))\n",
    "    g.add((Memb, DCSSO['xG90'], Literal(row['xG90'], datatype=XSD.double)))\n",
    "    g.add((Memb, DCSSO['xA90'], Literal(row['xA90'], datatype=XSD.double)))\n",
    "    g.add((Memb, DCSSO['npxG90'], Literal(row['npxG90'], datatype=XSD.double)))\n",
    "\n",
    "    #key pass. set to -1 implies that we miss the following data\n",
    "    if(row['key_passes'] != -1):\n",
    "        g.add((Memb, DCSSO['keyPasses'], Literal(row['key_passes'], datatype=XSD.int)))\n",
    "        g.add((Memb, DCSSO['xGBuildup'], Literal(row['xGBuildup'], datatype=XSD.double)))\n",
    "        g.add((Memb, DCSSO['xGChain'], Literal(row['xGChain'], datatype=XSD.double)))\n",
    "\n",
    "print(\"Players with value: \"+str(yes)+\" -- no value: \"+str(no))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- saving serialization ---\n",
      "CPU times: total: 1.06 s\n",
      "Wall time: 4.24 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# print all the data in the Turtle format\n",
    "print(\"--- saving serialization ---\")\n",
    "with open(savePath + 'stats'+shortSeas+'.ttl', 'w') as file:\n",
    "    file.write(g.serialize(format='turtle'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
